{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORgz4Rl/JhygGy1KTlh9+N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MS0C54073/-/blob/main/spike.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBSYG6S-vRw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7a6bde-3af1-4920-9725-54238c37d555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)] Loss: 2.458688 Accuracy: 7.50%\n",
            "Train Epoch: 1 [10000/60000 (17%)] Loss: 1.795413 Accuracy: 38.10%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_set_loader, optimizer, epoch, logging_interval=100):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_set_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % logging_interval == 0:\n",
        "            pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "            correct = pred.eq(target.view_as(pred)).float().mean().item()\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f} Accuracy: {:.2f}%'.format(\n",
        "                epoch, batch_idx * len(data), len(train_set_loader.dataset),\n",
        "                100. * batch_idx / len(train_set_loader), loss.item(),\n",
        "                100. * correct))\n",
        "\n",
        "def train_many_epochs(model):\n",
        "    epoch = 1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
        "    test(model, device, test_set_loader)\n",
        "\n",
        "    epoch = 2\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5)\n",
        "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
        "    test(model, device, test_set_loader)\n",
        "\n",
        "    epoch = 3\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
        "    test(model, device, test_set_loader)\n",
        "\n",
        "def test(model, device, test_set_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_set_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # Note: with `reduce=True`, I'm not sure what would happen with a final batch size\n",
        "            # that would be smaller than regular previous batch sizes. For now it works.\n",
        "            test_loss += F.nll_loss(output, target, reduce=True).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_set_loader.dataset)\n",
        "    print(\"\")\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss,\n",
        "        correct, len(test_set_loader.dataset),\n",
        "        100. * correct / len(test_set_loader.dataset)))\n",
        "    print(\"\")\n",
        "\n",
        "def download_mnist(data_path):\n",
        "    if not os.path.exists(data_path):\n",
        "        os.mkdir(data_path)\n",
        "    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "    training_set = torchvision.datasets.MNIST(data_path, train=True, transform=transformation, download=True)\n",
        "    testing_set = torchvision.datasets.MNIST(data_path, train=False, transform=transformation, download=True)\n",
        "    return training_set, testing_set\n",
        "batch_size = 1000\n",
        "DATA_PATH = './data'\n",
        "\n",
        "training_set, testing_set = download_mnist(DATA_PATH)\n",
        "train_set_loader = torch.utils.data.DataLoader(\n",
        "    dataset=training_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "test_set_loader = torch.utils.data.DataLoader(\n",
        "    dataset=testing_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "# Use GPU whever possible!\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "class SpikingNeuronLayerRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, device, n_inputs=28*28, n_hidden=100, decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5):\n",
        "        super(SpikingNeuronLayerRNN, self).__init__()\n",
        "        self.device = device\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.decay_multiplier = decay_multiplier\n",
        "        self.threshold = threshold\n",
        "        self.penalty_threshold = penalty_threshold\n",
        "\n",
        "        self.fc = nn.Linear(n_inputs, n_hidden)\n",
        "\n",
        "        self.init_parameters()\n",
        "        self.reset_state()\n",
        "        self.to(self.device)\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            if param.dim() >= 2:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.prev_inner = torch.zeros([self.n_hidden]).to(self.device)\n",
        "        self.prev_outer = torch.zeros([self.n_hidden]).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Call the neuron at every time step.\n",
        "\n",
        "        x: activated_neurons_below\n",
        "\n",
        "        return: a tuple of (state, output) for each time step. Each item in the tuple\n",
        "        are then themselves of shape (batch_size, n_hidden) and are PyTorch objects, such\n",
        "        that the whole returned would be of shape (2, batch_size, n_hidden) if casted.\n",
        "        \"\"\"\n",
        "        if self.prev_inner.dim() == 1:\n",
        "            # Adding batch_size dimension directly after doing a `self.reset_state()`:\n",
        "            batch_size = x.shape[0]\n",
        "            self.prev_inner = torch.stack(batch_size * [self.prev_inner])\n",
        "            self.prev_outer = torch.stack(batch_size * [self.prev_outer])\n",
        "\n",
        "        # 1. Weight matrix multiplies the input x\n",
        "        input_excitation = self.fc(x)\n",
        "\n",
        "        # 2. We add the result to a decayed version of the information we already had.\n",
        "        inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n",
        "\n",
        "        # 3. We compute the activation of the neuron to find its output value,\n",
        "        #    but before the activation, there is also a negative bias that refrain thing from firing too much.\n",
        "        outer_excitation = F.relu(inner_excitation - self.threshold)\n",
        "\n",
        "        # 4. If the neuron fires, the activation of the neuron is subtracted to its inner state\n",
        "        #    (and with an extra penalty for increase refractory time),\n",
        "        #    because it discharges naturally so it shouldn't fire twice.\n",
        "        do_penalize_gate = (outer_excitation > 0).float()\n",
        "        # TODO: remove following /2?\n",
        "        inner_excitation = inner_excitation - (self.penalty_threshold/self.threshold * inner_excitation) * do_penalize_gate\n",
        "\n",
        "        # 5. The outer excitation has a negative part after the positive part.\n",
        "        outer_excitation = outer_excitation #+ torch.abs(self.prev_outer) * self.decay_multiplier / 2.0\n",
        "\n",
        "        # 6. Setting internal values before returning.\n",
        "        #    And the returning value is the one of the previous time step to delay\n",
        "        #    activation of 1 time step of \"processing\" time. For logits, we don't take activation.\n",
        "        delayed_return_state = self.prev_inner\n",
        "        delayed_return_output = self.prev_outer\n",
        "        self.prev_inner = inner_excitation\n",
        "        self.prev_outer = outer_excitation\n",
        "        return delayed_return_state, delayed_return_output\n",
        "\n",
        "\n",
        "class InputDataToSpikingPerceptronLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        super(InputDataToSpikingPerceptronLayer, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.reset_state()\n",
        "        self.to(self.device)\n",
        "\n",
        "    def reset_state(self):\n",
        "        #     self.prev_state = torch.zeros([self.n_hidden]).to(self.device)\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, is_2D=True):\n",
        "        x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n",
        "        random_activation_perceptron = torch.rand(x.shape).to(self.device)\n",
        "        return random_activation_perceptron * x\n",
        "\n",
        "\n",
        "class OutputDataToSpikingPerceptronLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, average_output=True):\n",
        "        \"\"\"\n",
        "        average_output: might be needed if this is used within a regular neural net as a layer.\n",
        "        Otherwise, sum may be numerically more stable for gradients with setting average_output=False.\n",
        "        \"\"\"\n",
        "        super(OutputDataToSpikingPerceptronLayer, self).__init__()\n",
        "        if average_output:\n",
        "            self.reducer = lambda x, dim: x.sum(dim=dim)\n",
        "        else:\n",
        "            self.reducer = lambda x, dim: x.mean(dim=dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if type(x) == list:\n",
        "            x = torch.stack(x)\n",
        "        return self.reducer(x, 0)\n",
        "\n",
        "\n",
        "class SpikingNet(nn.Module):\n",
        "\n",
        "    def __init__(self, device, n_time_steps, begin_eval):\n",
        "        super(SpikingNet, self).__init__()\n",
        "        assert (0 <= begin_eval and begin_eval < n_time_steps)\n",
        "        self.device = device\n",
        "        self.n_time_steps = n_time_steps\n",
        "        self.begin_eval = begin_eval\n",
        "\n",
        "        self.input_conversion = InputDataToSpikingPerceptronLayer(device)\n",
        "\n",
        "        self.layer1 = SpikingNeuronLayerRNN(\n",
        "            device, n_inputs=28*28, n_hidden=100,\n",
        "            decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5\n",
        "        )\n",
        "\n",
        "        self.layer2 = SpikingNeuronLayerRNN(\n",
        "            device, n_inputs=100, n_hidden=10,\n",
        "            decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5\n",
        "        )\n",
        "\n",
        "        self.output_conversion = OutputDataToSpikingPerceptronLayer(average_output=False)  # Sum on outputs.\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward_through_time(self, x):\n",
        "        \"\"\"\n",
        "        This acts as a layer. Its input is non-time-related, and its output too.\n",
        "        So the time iterations happens inside, and the returned layer is thus\n",
        "        passed through global average pooling on the time axis before the return\n",
        "        such as to be able to mix this pipeline with regular backprop layers such\n",
        "        as the input data and the output data.\n",
        "        \"\"\"\n",
        "        self.input_conversion.reset_state()\n",
        "        self.layer1.reset_state()\n",
        "        self.layer2.reset_state()\n",
        "\n",
        "        out = []\n",
        "\n",
        "        all_layer1_states = []\n",
        "        all_layer1_outputs = []\n",
        "        all_layer2_states = []\n",
        "        all_layer2_outputs = []\n",
        "        for _ in range(self.n_time_steps):\n",
        "            xi = self.input_conversion(x)\n",
        "\n",
        "            # For layer 1, we take the regular output.\n",
        "            layer1_state, layer1_output = self.layer1(xi)\n",
        "\n",
        "            # We take inner state of layer 2 because it's pre-activation and thus acts as out logits.\n",
        "            layer2_state, layer2_output = self.layer2(layer1_output)\n",
        "\n",
        "            all_layer1_states.append(layer1_state)\n",
        "            all_layer1_outputs.append(layer1_output)\n",
        "            all_layer2_states.append(layer2_state)\n",
        "            all_layer2_outputs.append(layer2_output)\n",
        "            out.append(layer2_state)\n",
        "\n",
        "        out = self.output_conversion(out[self.begin_eval:])\n",
        "        return out, [[all_layer1_states, all_layer1_outputs], [all_layer2_states, all_layer2_outputs]]\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.forward_through_time(x)\n",
        "        return F.log_softmax(out, dim=-1)\n",
        "\n",
        "    def visualize_all_neurons(self, x):\n",
        "        assert x.shape[0] == 1 and len(x.shape) == 4, (\n",
        "            \"Pass only 1 example to SpikingNet.visualize(x) with outer dimension shape of 1.\")\n",
        "        _, layers_state = self.forward_through_time(x)\n",
        "\n",
        "        for i, (all_layer_states, all_layer_outputs) in enumerate(layers_state):\n",
        "            layer_state  =  torch.stack(all_layer_states).data.cpu().numpy().squeeze().transpose()\n",
        "            layer_output = torch.stack(all_layer_outputs).data.cpu().numpy().squeeze().transpose()\n",
        "\n",
        "            self.plot_layer(layer_state, title=\"Inner state values of neurons for layer {}\".format(i))\n",
        "            self.plot_layer(layer_output, title=\"Output spikes (activation) values of neurons for layer {}\".format(i))\n",
        "\n",
        "    def visualize_neuron(self, x, layer_idx, neuron_idx):\n",
        "        assert x.shape[0] == 1 and len(x.shape) == 4, (\n",
        "            \"Pass only 1 example to SpikingNet.visualize(x) with outer dimension shape of 1.\")\n",
        "        _, layers_state = self.forward_through_time(x)\n",
        "\n",
        "        all_layer_states, all_layer_outputs = layers_state[layer_idx]\n",
        "        layer_state  =  torch.stack(all_layer_states).data.cpu().numpy().squeeze().transpose()\n",
        "        layer_output = torch.stack(all_layer_outputs).data.cpu().numpy().squeeze().transpose()\n",
        "\n",
        "        self.plot_neuron(layer_state[neuron_idx], title=\"Inner state values neuron {} of layer {}\".format(neuron_idx, layer_idx))\n",
        "        self.plot_neuron(layer_output[neuron_idx], title=\"Output spikes (activation) values of neuron {} of layer {}\".format(neuron_idx, layer_idx))\n",
        "\n",
        "    def plot_layer(self, layer_values, title):\n",
        "        width = max(16, layer_values.shape[0] / 8)\n",
        "        height = max(4, layer_values.shape[1] / 8)\n",
        "        plt.figure(figsize=(width, height))\n",
        "        plt.imshow(\n",
        "            layer_values,\n",
        "            interpolation=\"nearest\",\n",
        "            cmap=plt.cm.rainbow\n",
        "        )\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Neurons of layer\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_neuron(self, neuron_through_time, title):\n",
        "        width = max(16, len(neuron_through_time) / 8)\n",
        "        height = 4\n",
        "        plt.figure(figsize=(width, height))\n",
        "        plt.title(title)\n",
        "        plt.plot(neuron_through_time)\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Neuron's activation\")\n",
        "        plt.show()\n",
        "\n",
        "class NonSpikingNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NonSpikingNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(28*28, 100)\n",
        "        self.layer2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x, is_2D=True):\n",
        "        x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x =        self.layer2(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "#Training a Spiking Neural Network (SNN)\n",
        "#Let's use our SpikingNet!\n",
        "spiking_model = SpikingNet(device, n_time_steps=128, begin_eval=0)\n",
        "train_many_epochs(spiking_model)\n",
        "\n",
        "#2\n",
        "non_spiking_model = NonSpikingNet().to(device)\n",
        "train_many_epochs(non_spiking_model)\n",
        "#3\n",
        "data, target = test_set_loader.__iter__().__next__()\n",
        "\n",
        "# taking 1st testing example:\n",
        "x = torch.stack([data[0]])\n",
        "y = target.data.numpy()[0]\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(x.data.cpu().numpy()[0,0])\n",
        "plt.title(\"Input image x of label y={}:\".format(y))\n",
        "plt.show()\n",
        "\n",
        "# plotting neuron's activations:\n",
        "spiking_model.visualize_all_neurons(x)\n",
        "print(\"A hidden neuron that looks excited:\")\n",
        "spiking_model.visualize_neuron(x, layer_idx=0, neuron_idx=0)\n",
        "print(\"The output neuron of the label:\")\n",
        "spiking_model.visualize_neuron(x, layer_idx=1, neuron_idx=y)\n",
        "\n",
        "\n",
        "#UnitTest code\n",
        "\n",
        "\n",
        "\n",
        "# Define the model classes and functions\n",
        "\n",
        "# ...\n",
        "\n",
        "# Define the unit tests\n",
        "\n",
        "'''\n",
        "class TestTraining(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        # Set up the test data and model for all test methods\n",
        "        cls.batch_size = 1000\n",
        "        cls.DATA_PATH = './data'\n",
        "\n",
        "        training_set, testing_set = download_mnist(cls.DATA_PATH)\n",
        "        cls.train_set_loader = torch.utils.data.DataLoader(\n",
        "            dataset=training_set,\n",
        "            batch_size=cls.batch_size,\n",
        "            shuffle=True)\n",
        "        cls.test_set_loader = torch.utils.data.DataLoader(\n",
        "            dataset=testing_set,\n",
        "            batch_size=cls.batch_size,\n",
        "            shuffle=False)\n",
        "\n",
        "        cls.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        cls.model = SpikingNet(cls.device, n_time_steps=3, begin_eval=1).to(cls.device)\n",
        "\n",
        "    def test_train(self):\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.5)\n",
        "        train(self.model, self.device, self.train_set_loader, optimizer, epoch=1, logging_interval=100)\n",
        "\n",
        "        # Assert that the model is in training mode\n",
        "        self.assertTrue(self.model.training)\n",
        "\n",
        "    def test_test(self):\n",
        "        test(self.model, self.device, self.test_set_loader)\n",
        "\n",
        "        # Assert that the model is in evaluation mode\n",
        "        self.assertFalse(self.model.training)\n",
        "\n",
        "class TestSpikingNeuronLayerRNN(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_inputs = 10\n",
        "        self.n_hidden = 5\n",
        "        self.model = SpikingNeuronLayerRNN(self.device, n_inputs=self.n_inputs, n_hidden=self.n_hidden)\n",
        "\n",
        "    def test_forward(self):\n",
        "        x = torch.randn(32, self.n_inputs).to(self.device)\n",
        "        state, output = self.model.forward(x)\n",
        "\n",
        "        # Assert the shapes of the state and output tensors\n",
        "        self.assertEqual(state.shape, (32, self.n_hidden))\n",
        "        self.assertEqual(output.shape, (32, self.n_hidden))\n",
        "\n",
        "    def test_reset_state(self):\n",
        "        self.model.reset_state()\n",
        "\n",
        "        # Assert that the previous inner and outer states are all zeros\n",
        "        self.assertTrue(torch.all(self.model.prev_inner == 0))\n",
        "        self.assertTrue(torch.all(self.model.prev_outer == 0))\n",
        "\n",
        "class TestInputDataToSpikingPerceptronLayer(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = InputDataToSpikingPerceptronLayer(self.device)\n",
        "\n",
        "    def test_forward(self):\n",
        "        x = torch.randn(32, 10, 28, 28).to(self.device)\n",
        "        output = self.model.forward(x)\n",
        "\n",
        "        # Assert the shape of the output tensor\n",
        "        self.assertEqual(output.shape, (32, 10, 28, 28))\n",
        "\n",
        "class TestOutputDataToSpikingPerceptronLayer(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.average_output = True\n",
        "        self.model = OutputDataToSpikingPerceptronLayer(average_output=self.average_output)\n",
        "\n",
        "    def test_forward(self):\n",
        "        x = [torch.randn(32, 10) for _ in range(3)]\n",
        "        output = self.model.forward(x)\n",
        "\n",
        "        # Assert the shape of the output tensor\n",
        "        self.assertEqual(output.shape, (32, 10))\n",
        "\n",
        "class TestSpikingNet(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = SpikingNet(self.device, n_time_steps=3, begin_eval=1).to(self.device)\n",
        "\n",
        "    def test_forward(self):\n",
        "        x = torch.randn(32, 1, 28, 28).to(self.device)\n",
        "        output = self.model.forward(x)\n",
        "\n",
        "        # Assert the shape of the output tensor\n",
        "        self.assertEqual(output.shape, (32, 10))\n",
        "\n",
        "class TestDownloadMNIST(unittest.TestCase):\n",
        "\n",
        "    def test_download_mnist(self):\n",
        "        data_path = './data'\n",
        "        training_set, testing_set = download_mnist(data_path)\n",
        "\n",
        "        # Check if the datasets are downloaded and loaded correctly\n",
        "        self.assertTrue(len(training_set) > 0)\n",
        "        self.assertTrue(len(testing_set) > 0)\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "    #unittest.main()\n",
        "#'''"
      ]
    }
  ]
}